{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Micrograds, Random, StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Random.seed!(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP of [Layer of [relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2), relu Neuron(2)], Layer of [relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16), relu Neuron(16)], Layer of [Linear Neuron(16)]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MLP(2, [16, 16, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"number of parameters\", length(parameters(model))) = (\"number of parameters\", 337)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"number of parameters\", 337)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show \"number of parameters\", length(parameters(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "function loss(batch_size=nothing)\n",
    "    if isnothing(batch_size) \n",
    "        Xb, yb = X, y\n",
    "    else\n",
    "        rindx = randperm(length(y))[1:batch_size]\n",
    "        Xb, yb = X[rindx,:], y[rindx]\n",
    "    end\n",
    "\n",
    "    valinput = Value.(Xb)\n",
    "    # mapslice?\n",
    "    scores = model.(eachrow(Xb))\n",
    "\n",
    "    losses = @. relu(1. + -yb * scores)\n",
    "    data_loss = sum(losses) * (1. / length(losses))\n",
    "    #l2 regularization\n",
    "    α = 1e-4\n",
    "    reg_loss = α * sum(parameters(model).^2)\n",
    "    total_loss = data_loss + reg_loss\n",
    "\n",
    "    accuracy = (yb .> 0) .== (score .> 0)\n",
    "    \n",
    "    total_loss, accuracy/length(accuracy)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization\n",
    "for k in 1:100\n",
    "    total_loss, acc = loss()\n",
    "    zero_grad(model)\n",
    "    total_loss.backward()\n",
    "\n",
    "    learning_rate = 1 - 0.9*k/100\n",
    "    for p in parameters(model)\n",
    "        p.data -= learning_rate * p.grad\n",
    "    end\n",
    "    \n",
    "\n",
    "    k % 1 == 0 && print(\"step $(k) loss: $(total_loss), accuracy: $(acc)\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
